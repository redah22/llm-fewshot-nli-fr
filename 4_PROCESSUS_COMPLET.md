# ğŸ“˜ Guide Complet du Processus - TER French NLI

## ğŸ¯ Vue d'Ensemble

Ce guide vous accompagne du dÃ©but Ã  la fin de votre projet TER.

## ğŸ“… Processus en 5 Ã‰tapes

### Ã‰tape 1: Setup & Exploration (1 semaine)

**Objectif**: PrÃ©parer l'environnement et comprendre les donnÃ©es

#### Actions:
```bash
# 1. Installation
source venv/bin/activate
pip install -r requirements.txt

# 2. Tester le chargement
python3 test_fracas.py
```

#### Notebook:
ğŸ““ **`notebooks/01_fracas_exploration.ipynb`**
- Charger FraCaS
- Diviser en train/val/test (60/20/20)
- Analyser les statistiques
- Visualiser les distributions
- **âš ï¸ Sauvegarder les splits!**

**Livrables**:
- [x] Environnement installÃ©
- [x] Dataset explorÃ©
- [x] Splits crÃ©Ã©s et sauvegardÃ©s
- [x] Statistiques documentÃ©es

---

### Ã‰tape 2: Baseline & Few-Shot sur VALIDATION (1-2 semaines)

**Objectif**: Tester different nombres de few-shot **sur validation**

> **âš ï¸ CRITIQUE**: On utilise VALIDATION, PAS test!

#### Pourquoi Validation?

```python
# âœ… CORRECT - Comparer sur validation
train_data = load_from_disk('data/processed/fracas_split')['train']
val_data = load_from_disk('data/processed/fracas_split')['validation']

# Exemples few-shot depuis train
few_shot_5 = train_data.select(range(5))

# Tester 0-shot sur validation
val_acc_0shot = evaluate_zero_shot(model, val_data)
print(f"0-shot (Val): {val_acc_0shot:.2f}")

# Tester 5-shot sur validation
val_acc_5shot = evaluate_few_shot(model, val_data, few_shot_5)
print(f"5-shot (Val): {val_acc_5shot:.2f}")

# âœ… Pas de leakage car on n'a pas touchÃ© test!
```

#### Notebook ou Script:
ğŸ““ **`notebooks/02_few_shot_validation.ipynb`** (dÃ©veloppement)  
OU  
ğŸ **`scripts/run_few_shot.py`** (expÃ©riences reproductibles)

```python
# Exemple de comparaison
num_shots = [0, 1, 3, 5, 10]
val_results = []

for n in num_shots:
    # Few-shot depuis train
    examples = train_data.select(range(n)) if n > 0 else None
    
    # Ã‰valuer sur validation
    val_acc = evaluate(model, val_data, examples)
    val_results.append(val_acc)
    
    print(f"{n}-shot: Validation accuracy = {val_acc:.2%}")

# RÃ©sultat: vous voyez l'amÃ©lioration sans toucher test!
```

**Livrables**:
- [x] RÃ©sultats 0-shot sur validation
- [x] RÃ©sultats few-shot (1, 3, 5, 10) sur validation  
- [x] Graphique comparatif
- [x] Meilleur nombre de shots identifiÃ©

---

### Ã‰tape 3: Fine-Tuning (Optionnel, 1-2 semaines)

**Objectif**: EntraÃ®ner CamemBERT sur train, monitorer sur validation

#### Script:
ğŸ **`scripts/fine_tune_camembert.py`**

```python
from transformers import Trainer, TrainingArguments

# Configuration
training_args = TrainingArguments(
    output_dir='checkpoints/fracas',
    num_train_epochs=3,
    evaluation_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
)

# EntraÃ®ner
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,      # âœ… Train
    eval_dataset=val_data,         # âœ… Validation
    # PAS test_data!
)

trainer.train()

# Voir les rÃ©sultats sur validation
val_results = trainer.evaluate(val_data)
print(f"Validation accuracy: {val_results['eval_accuracy']:.2%}")
```

**Livrables**:
- [x] ModÃ¨le entraÃ®nÃ©
- [x] Courbes d'apprentissage (train/val)
- [x] Meilleur checkpoint sauvegardÃ©
- [x] Performance sur validation

---

### Ã‰tape 4: Analyse des RÃ©sultats sur VALIDATION (1 semaine)

**Objectif**: Comparer toutes les approches sur validation

#### Notebook:
ğŸ““ **`notebooks/03_results_analysis.ipynb`**

```python
import pandas as pd
import matplotlib.pyplot as plt

# Compiler les rÃ©sultats sur VALIDATION
results = pd.DataFrame({
    'Approche': ['0-shot', '1-shot', '3-shot', '5-shot', '10-shot', 'Fine-tuned'],
    'Val_Accuracy': [0.45, 0.52, 0.61, 0.68, 0.71, 0.75]  # Vos rÃ©sultats
})

# Visualiser
plt.figure(figsize=(10, 6))
plt.bar(results['Approche'], results['Val_Accuracy'])
plt.ylabel('Validation Accuracy')
plt.title('Comparaison des Approches (sur Validation)')
plt.ylim(0, 1)
plt.show()

# âœ… Tout est fait sur validation, test reste intact!
```

**Livrables**:
- [x] Tableau comparatif
- [x] Graphiques
- [x] Analyse d'erreurs
- [x] Meilleure approche identifiÃ©e

---

### Ã‰tape 5: Ã‰valuation Finale sur TEST (1 jour)

**Objectif**: Rapporter les rÃ©sultats finaux

> **âš ï¸ UNE SEULE FOIS! Ne plus modifier aprÃ¨s!**

#### Script:
ğŸ **`scripts/final_evaluation.py`**

```python
from datasets import DatasetDict

# Charger les splits
fracas = DatasetDict.load_from_disk('data/processed/fracas_split')
test_data = fracas['test']

# Charger le meilleur modÃ¨le (dÃ©cidÃ© sur validation)
best_model = load_best_model('checkpoints/fracas/best/')

# âœ… Ã‰valuation finale sur TEST (premiÃ¨re et derniÃ¨re fois!)
test_results = evaluate(best_model, test_data)

print("="*60)
print("ğŸ¯ RÃ‰SULTATS FINAUX")
print("="*60)
print(f"Test Accuracy: {test_results['accuracy']:.2%}")
print(f"Test F1-Macro: {test_results['f1_macro']:.2%}")
print("="*60)
print("âš ï¸  Ces rÃ©sultats sont Ã  rapporter dans le TER")
print("âš ï¸  NE PLUS MODIFIER le modÃ¨le!")
print("="*60)
```

**Livrables**:
- [x] RÃ©sultats test finaux
- [x] Rapport TER
- [x] (Optionnel) Publication

---

## ğŸ“Š RÃ©ponse Ã  Votre Question

### "On peut comparer avant/aprÃ¨s few-shot?"

**OUI, MAIS sur VALIDATION!**

```python
# âœ… CORRECT - DÃ©veloppement sur validation
val_data = load_validation()

# Baseline (0-shot)
acc_0 = evaluate_zero_shot(model, val_data)  # Ex: 45%

# Few-shot (5 exemples)
acc_5 = evaluate_few_shot(model, val_data, n=5)  # Ex: 68%

print(f"AmÃ©lioration: +{acc_5 - acc_0:.0%}")  # Ex: +23%

# âœ… Pas de leakage! On compare sur validation.
```

```python
# âŒ INCORRECT - NE PAS FAIRE Ã‡A!
test_data = load_test()  # âŒ Test!

acc_0 = evaluate_zero_shot(model, test_data)  # âŒ Leakage!
acc_5 = evaluate_few_shot(model, test_data, n=5)  # âŒ  Leakage!

# âŒ RÃ©sultats biaisÃ©s car vous avez vu test!
```

### Workflow Correct:

```
1. DÃ‰VELOPPEMENT (train + validation)
   â”œâ”€ Tester 0-shot sur validation â†’ 45%
   â”œâ”€ Tester few-shot sur validation â†’ 68%
   â””â”€ Choisir le meilleur (ex: 5-shot)

2. Ã‰VALUATION FINALE (test - UNE FOIS)
   â””â”€ Tester 5-shot sur test â†’ 65% (rÃ©sultat final)
```

**Pourquoi validation < test parfois?**
- Normal! Variance entre splits
- L'important: pas de leakage
- RÃ©sultats test = rÃ©sultats officiels

---

## ğŸ—‚ï¸ Organisation des Fichiers

```
TER_M1/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_fracas_exploration.ipynb      # Ã‰tape 1
â”‚   â”œâ”€â”€ 02_few_shot_validation.ipynb     # Ã‰tape 2
â”‚   â””â”€â”€ 03_results_analysis.ipynb        # Ã‰tape 4
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ test_fracas.py                   # Test rapide
â”‚   â”œâ”€â”€ run_few_shot.py                  # ExpÃ©riences few-shot
â”‚   â”œâ”€â”€ fine_tune_camembert.py           # Fine-tuning
â”‚   â””â”€â”€ final_evaluation.py              # Ã‰valuation finale
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ fracas_split/                # Splits sauvegardÃ©s
â”‚           â”œâ”€â”€ train/
â”‚           â”œâ”€â”€ validation/
â”‚           â””â”€â”€ test/
â”‚
â””â”€â”€ results/
    â”œâ”€â”€ few_shot_validation.json         # RÃ©sultats validation
    â””â”€â”€ final_test_results.json          # RÃ©sultats test
```

---

## âœ… Checklist ComplÃ¨te

### Ã‰tape 1: Setup
- [x] Environnement installÃ©
- [x] FraCaS chargÃ©
- [x] Notebook exploration exÃ©cutÃ©
- [x] Splits crÃ©Ã©s et sauvegardÃ©s

### Ã‰tape 2: Validation
- [ ] 0-shot testÃ© sur validation
- [ ] Few-shot testÃ© sur validation (1, 3, 5, 10)
- [ ] Graphique comparatif crÃ©Ã©
- [ ] Meilleur n_shots choisi

### Ã‰tape 3: Fine-Tuning (optionnel)
- [ ] ModÃ¨le entraÃ®nÃ© sur train
- [ ] Monitored sur validation
- [ ] Checkpoint sauvegardÃ©

### Ã‰tape 4: Analyse
- [ ] RÃ©sultats compilÃ©s
- [ ] Figures crÃ©Ã©es
- [ ] Analyse d'erreurs faite

### Ã‰tape 5: Test Final
- [ ] ModÃ¨le finalisÃ©
- [ ] Test Ã©valuÃ© (UNE FOIS)
- [ ] RÃ©sultats rapportÃ©s
- [ ] Rapport TER rÃ©digÃ©

---

## ğŸ¯ En RÃ©sumÃ©

1. **Explorez** avec `01_fracas_exploration.ipynb`
2. **DÃ©veloppez** sur validation (comparez 0-shot vs few-shot)
3. **Analysez** les rÃ©sultats validation
4. **Ã‰valuez** sur test (une fois!)
5. **Rapportez** les rÃ©sultats test

**Test = compÃ©tition finale ğŸ†**  
**Validation = terrain d'entraÃ®nement âš½**

Vous voyez l'amÃ©lioration sans tricher! ğŸ‰
